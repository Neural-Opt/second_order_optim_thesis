% Encoding: windows-1252

@book{nocedal2006numerical,
  title={Numerical Optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  publisher={Springer},
  edition={2nd},
  year={2006},
  isbn={978-0-387-30303-1}
}
@misc{pytorch_dataparallel,
  title = {torch.nn.DataParallel},
  author = {PyTorch},
  year = {2023},
  howpublished = {\url{https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html}},
  note = {Accessed: 2024-07-01}
}
@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A.},
  journal={Neural Computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press},
  doi={10.1162/neco.1994.6.1.147},
  url={https://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf}
}

@misc{anil2021scalable,
      title={Scalable Second Order Optimization for Deep Learning}, 
      author={Rohan Anil and Vineet Gupta and Tomer Koren and Kevin Regan and Yoram Singer},
      year={2021},
      eprint={2002.09018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{yao2021adahessian,
      title={ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning}, 
      author={Zhewei Yao and Amir Gholami and Sheng Shen and Mustafa Mustafa and Kurt Keutzer and Michael W. Mahoney},
      year={2021},
      eprint={2006.00719},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{apollo,
      title={Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization}, 
      author={Xuezhe Ma},
      year={2021},
      eprint={2009.13586},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.13586}, 
}
@misc{kashyap2023survey,
      title={A survey of deep learning optimizers -- first and second order methods}, 
      author={Rohan Kashyap},
      year={2023},
      eprint={2211.15596},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{sun2019survey,
      title={A Survey of Optimization Methods from a Machine Learning Perspective}, 
      author={Shiliang Sun and Zehui Cao and Han Zhu and Jing Zhao},
      year={2019},
      eprint={1906.06821},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{newtonsmethod,
  author = {Ryan Tibshirani},
  title = {Lecture 14: March 2: Newton’s Method},
  year = {2015},
  url = {https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/14-newton-scribed.pdf},
  note = {Lecture notes from a convex optimization course at Carnegie Mellon University.}
}
@misc{linesearch,
    title={Choosing Step Size for Large-Scale Optimization},
    author={Costis},
    note={Available online: \url{https://scicomp.stackexchange.com/a/455/29}},
    year={2012}
}
@book{bachman2007,
    title={Advanced Calculus Demystified},
    author={Bachman, David},
    year={2007},
    publisher={McGraw-Hill Education},
    isbn={978-0-07-147217-3}
}
@misc{toussain2014gradient,
  author = {Marc Toussaint},
  title = {Gradient Descent - Algorithm, History, and Applications},
  year = {2014},
  howpublished = {\url{https://www.user.tu-berlin.de/mtoussai/notes/gradientDescent.pdf}},
  note = {Accessed: 2024-05-08}
}
@misc{tiny_imagenet,
  title = {Tiny ImageNet Dataset},
  year = {2023},
  url = {https://paperswithcode.com/dataset/tiny-imagenet},
  note = {Accessed: 2024-07-06}
}

@misc{wu2020wngrad,
      title={WNGrad: Learn the Learning Rate in Gradient Descent}, 
      author={Xiaoxia Wu and Rachel Ward and Léon Bottou},
      year={2020},
      eprint={1803.02865},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{bottou2018optimization,
      title={Optimization Methods for Large-Scale Machine Learning}, 
      author={Léon Bottou and Frank E. Curtis and Jorge Nocedal},
      year={2018},
      eprint={1606.04838},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{stanfordSGD,
  author       = {Stanford University},
  title        = {Optimization: Stochastic Gradient Descent},
  howpublished = {\url{http://deeplearning.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/}},
  note         = {Accessed: 2024-05-08}
}
@misc{garrigos2024handbook,
      title={Handbook of Convergence Theorems for (Stochastic) Gradient Methods}, 
      author={Guillaume Garrigos and Robert M. Gower},
      year={2024},
      eprint={2301.11235},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@book{GrundkursAI,
  author    = {Wolfgang Ertel},
  title     = {Grundkurs Künstliche Intelligenz: Eine praxisorientierte Einführung},
  publisher = {Springer Vieweg},
  year      = {2021},
  edition   = {5},
  isbn      = {978-3-658-33723-5},
  language  = {German},
}
@article{McCullochPitts1943,
  author    = {McCulloch, Warren S. and Pitts, Walter},
  title     = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  journal   = {The Bulletin of Mathematical Biophysics},
  volume    = {5},
  number    = {4},
  pages     = {115-133},
  year      = {1943},
  doi       = {10.1007/BF02478259},
}
@article{Rosenblatt1958,
  author    = {Rosenblatt, Frank},
  title     = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal   = {Psychological Review},
  volume    = {65},
  number    = {6},
  pages     = {386-408},
  year      = {1958},
  doi       = {10.1037/h0042519},
}
@book{MinskyPapert1969,
  author    = {Minsky, Marvin and Papert, Seymour},
  title     = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {MIT Press},
  year      = {1969},
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{BEKAS20071214,
title = {An estimator for the diagonal of a matrix},
journal = {Applied Numerical Mathematics},
volume = {57},
number = {11},
pages = {1214-1229},
year = {2007},
note = {Numerical Algorithms, Parallelism and Applications (2)},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0168927407000244},
author = {C. Bekas and E. Kokiopoulou and Y. Saad},
keywords = {Stochastic estimator, Hadamard matrices, Grassmannian spaces, Electronic structure calculations, Density Functional Theory},
abstract = {A number of applications require to compute an approximation of the diagonal of a matrix when this matrix is not explicitly available but matrix–vector products with it are easy to evaluate. In some cases, it is the trace of the matrix rather than the diagonal that is needed. This paper describes methods for estimating diagonals and traces of matrices in these situations. The goal is to obtain a good estimate of the diagonal by applying only a small number of matrix–vector products, using selected vectors. We begin by considering the use of random test vectors and then explore special vectors obtained from Hadamard matrices. The methods are tested in the context of computational materials science to estimate the diagonal of the density matrix which holds the charge densities. Numerical experiments indicate that the diagonal estimator may offer an alternative method that in some cases can greatly reduce computational costs in electronic structures calculations.}
}
@misc{lu2017expressive,
      title={The Expressive Power of Neural Networks: A View from the Width}, 
      author={Zhou Lu and Hongming Pu and Feicheng Wang and Zhiqiang Hu and Liwei Wang},
      year={2017},
      eprint={1709.02540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{universal_approx_theorem_2023,
  author = {Deep Mind},
  title = {The Universal Approximation Theorem},
  year = {2023},
  url = {https://www.deep-mind.org/2023/03/26/the-universal-approximation-theorem/},
  note = {Accessed: 2024-05-18}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}
@article{IBMRegularization,
  title = {What Is Regularization?},
  author = {Jacob Murel, Ph.D. and Eda Kavlakoglu},
  year = {2023},
  url = {https://www.ibm.com/topics/regularization},
  note = {Accessed: 2024-05-27}
}
@techreport{Bishop1992,
  title = {Exact Calculation of the Hessian Matrix for the Multilayer Perceptron},
  author = {Christopher M. Bishop},
  year = {1992},
  institution = {Neural Computing Research Group, Aston University},
  url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-hessian-nc-92.pdf},
  note = {Accessed: 2024-05-27}
}
@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T.},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}
@techreport{hintonrms,
  title = {Neural	Networks	for	Machine	Learning},
  author = {GeoffreyHinton},
  year = {2012},
  institution = {Neural Computing Research Group, Aston University},
  url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  note = {Accessed: 2024-06-04}
}
@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{d2l2023adam,
  title = {Adam},
  author = {Dive into Deep Learning},
  year = {2023},
  publisher = {D2L.ai},
  howpublished = {\url{https://d2l.ai/chapter_optimization/adam.html}},
  note = {Accessed: 2024-06-10}
}

@book{strang2022introduction,
  title={Introduction to linear algebra},
  author={Strang, Gilbert},
  year={2022},
  publisher={SIAM}
}
@misc{zhuang2020adabeliefoptimizeradaptingstepsizes,
      title={AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients}, 
      author={Juntang Zhuang and Tommy Tang and Yifan Ding and Sekhar Tatikonda and Nicha Dvornek and Xenophon Papademetris and James S. Duncan},
      year={2020},
      eprint={2010.07468},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2010.07468}, 
}
@inproceedings{zhuang2020adabelief,
  title={AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients},
  author={Juntang Zhuang and Tommy Tang and Yifan Ding and Sekhar Tatikonda and Nicha Dvornek and Xenophon Papademetris and James S. Duncan},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  url={https://juntang-zhuang.github.io/adabelief/},
}
@article{Loshchilov2017FixingWD,
  author       = {Ilya Loshchilov and
                  Frank Hutter},
  title        = {Fixing Weight Decay Regularization in Adam},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.05101},
  eprinttype   = {arXiv},
  eprint       = {1711.05101}
}
@misc{DecoupledWeightDecay,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}
@misc{Resnet110,
  author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian Q.},
  title = {Deep Networks with Stochastic Depth},
  year = {2016},
  howpublished = {\url{https://www.researchgate.net/figure/Comparison-of-ResNets-with-110-and-1202-layers-When-trained-with-stochastic-depth-the_fig4_301879329}},
  note = {[Figure] Accessed 16 Jul 2024},
  publisher = {ResearchGate}
}
@article{Zhu1999TheQR,
  title={The Quasi-Cauchy Relation and Diagonal Updating},
  author={M. Zhu and John Lawrence Nazareth and Henry Wolkowicz},
  journal={SIAM J. Optim.},
  year={1999},
  volume={9},
  pages={1192-1204},
  url={https://api.semanticscholar.org/CorpusID:949459}
}

@article{DFP,
  title={A rapidly convergent descent method for minimization},
  author={Fletcher, Roger and Powell, Michael JD},
  journal={The Computer Journal},
  volume={6},
  number={2},
  pages={163--168},
  year={1963},
  publisher={Oxford University Press}
}

@article{BFGS,
  title={The convergence of a class of double-rank minimization algorithms 1. general considerations},
  author={Broyden, Charles George},
  journal={IMA Journal of Applied Mathematics},
  volume={6},
  number={1},
  pages={76--90},
  year={1970},
  publisher={Oxford University Press}
}
@article{SR1,
  title={Quasi-Newton methods and their application to function minimization},
  author={Broyden, Charles George},
  journal={Mathematics of Computation},
  volume={21},
  number={99},
  pages={368--381},
  year={1967},
  publisher={American Mathematical Society}
}
@article{HessianDiagonal,
title = {An estimator for the diagonal of a matrix},
journal = {Applied Numerical Mathematics},
volume = {57},
number = {11},
pages = {1214-1229},
year = {2007},
note = {Numerical Algorithms, Parallelism and Applications (2)},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0168927407000244},
author = {C. Bekas and E. Kokiopoulou and Y. Saad},
keywords = {Stochastic estimator, Hadamard matrices, Grassmannian spaces, Electronic structure calculations, Density Functional Theory},
abstract = {A number of applications require to compute an approximation of the diagonal of a matrix when this matrix is not explicitly available but matrix–vector products with it are easy to evaluate. In some cases, it is the trace of the matrix rather than the diagonal that is needed. This paper describes methods for estimating diagonals and traces of matrices in these situations. The goal is to obtain a good estimate of the diagonal by applying only a small number of matrix–vector products, using selected vectors. We begin by considering the use of random test vectors and then explore special vectors obtained from Hadamard matrices. The methods are tested in the context of computational materials science to estimate the diagonal of the density matrix which holds the charge densities. Numerical experiments indicate that the diagonal estimator may offer an alternative method that in some cases can greatly reduce computational costs in electronic structures calculations.}
}

@article{hutchinson,
  title={A stochastic estimator of the trace of the influence matrix for {L}aplacian smoothing splines},
  author={Hutchinson, Michael F},
  journal={Communications in Statistics-Simulation and Computation},
  volume={19},
  number={2},
  pages={433--450},
  year={1990},
  publisher={Taylor \& Francis},
  doi={10.1080/03610919008812866}
}

@misc{convexity_smoothness,
  author = {Wang, X.},
  title = {Optimization Basics},
  year = {2021},
  howpublished = {Lecture slides},
  institution = {Department of Statistics, Purdue University},
  url = {https://www.stat.purdue.edu/~wang4094/resources/slides/2021_spring_DL_meeting_01_opt_basics.pdf},
  note = {Accessed: July 2024}
}
@book{boyd2004convex,
  title={Convex Optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge University Press},
  address={Cambridge, UK},
  isbn={9780521833783}
}
@misc{AttentionIsAllUNeed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}
@book{arens2011mathematik,
  title     = {Mathematik},
  author    = {Arens, Tilo and Hettlich, Frank and Karpfinger, Christian and Kockelkorn, Ulrich and Lichtenegger, Klaus and Stachel, Hellmuth},
  edition   = {2},
  year      = {2011},
  pages     = {1506},
  publisher = {Spektrum Akademischer Verlag},
  address   = {Heidelberg},
  isbn      = {978-3-8274-2347-4},
}

@incollection{SZABO2015320,
title = {S},
editor = {Fred E. Szabo},
booktitle = {The Linear Algebra Survival Guide},
publisher = {Academic Press},
address = {Boston},
pages = {320-377},
year = {2015},
isbn = {978-0-12-409520-5},
doi = {https://doi.org/10.1016/B978-0-12-409520-5.50026-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095205500266},
author = {Fred E. Szabo}
}
@inproceedings{nair2010rectified,
  title={Rectified linear units improve restricted Boltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}
@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M},
  year={2006},
  publisher={Springer}
}
@misc{dao2020understanding,
  author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher},
  title = {Understanding the Difficulty of Training Transformers},
  year = {2020},
  eprint = {2004.08249},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://ar5iv.labs.arxiv.org/html/2004.08249}
}

@misc{jaketae_fisher,
  author = {Jake Tae},
  title = {Fisher Information and Its Applications},
  url = {https://jaketae.github.io/study/fisher/},
  year = {2021},
  note = {Accessed: 2024-09-13},
}
