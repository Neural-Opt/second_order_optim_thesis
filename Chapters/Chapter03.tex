%************************************************
\chapter{The Hessian in Neural Networks}\label{ch:mathtest} % $\mathbb{ZNR}$
%************************************************
In this chapter, we will closely examine the behavior of \emph{Apollo} and \emph{AdaHessian} to understand how they differ from established first-order methods and whether their resource overhead is justified.
As both \emph{Apollo} and \emph{AdaHessian} claim to provide more accurate second-order estimates
than their predecessors, we therefore compare the calculated batch Hessian
diagonal with the approximations generated by \emph{Apollo}, \emph{AdaHessian}, \emph{Adam}, and \emph{AdaBelief}
across different batch sizes.

\section{Hessian Approximization Quality}
Before comparing the quality of the Hessian approximations provided by the previously mentioned optimizers,
 we need to address some additional details.
Recalling their definitions, \emph{Adam}, \emph{AdaBelief}, and \emph{AdaHessian} all use an estimate of the
absolute curvature in their second-moment computations.
The diagonal elements of the Hessian matrix of the loss function consist of the second-order partial derivatives with respect to each parameter $H_{ii} = \frac{\partial^2 L}{\partial \theta_i^2},$
where \( L \) is the loss function and \( \theta_i \) is the \( i \)-th parameter.
The entries \( H_{ii} \) are negative when the loss function is locally concave with respect to \( \theta_i \) and positive when it is locally convex.
Consequently, we will compare the \emph{absolute} values of the Hessian diagonal elements with the optimizers' approximations,
 since they only consider the \emph{absolute} curvatureâ€”that is, they ignore the sign of the Hessian diagonal entries, 
 as they all use the squared approaximations in their second moment.
To do so, we first calculate the Hessian diagonal for the current batch using \texttt{torch.autograd}.
Since PyTorch does not natively support the calculation of the Hessian diagonal in a fully vectorized form,
we implemented this by iterating over each gradient element, performing a second backward pass, and extracting the corresponding second-order derivative.
Given that this process is inherently slow, we limited our investigation to a relatively small CNN model with 13.5K parameters, consisting of two convolutional layers and two fully connected layers.
For training, we used the MNIST dataset, which contains 60,000 images of handwritten digits, each of size 32 $\times$ 32.
To measure the similarity between the approximated Hessian diagonal and the actual batch Hessian diagonal, we needed a metric that is independent of the magnitude of the vectors.
This is crucial because the scale of the second moments can be adjusted through the learning rate.
Therefore, we utilized the cosine similarity measure, which is defined as follows:
\[
\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}, \quad \mathbf{A}, \mathbf{B} \in \mathbb{R}^n.
\]
This provides a measure of the angle between the two vectors, where \( \cos(\theta) = 1 \) indicates perfect directional alignment, and \( \cos(\theta) = 0 \) indicates orthogonality.
We record the measured cosine similarity and plot its development throughout the training process.
This is done for both batch sizes of 128 and 1024 to observe whether the increased stochastic variance in the gradient significantly impacts the quality of the approximation results.
In Figures \ref{fig:cosine-small-batch} and \ref{fig:cosine-big-batch}, we observe the angle between the approximations
and the \emph{absolute} batch Hessian diagonal for both small and large batch sizes.
For improved visualization, a moving average with a window size of 10 is applied to the small batch plots.
The results indicate that \emph{Apollo}'s approximations are much worse than those of the other optimizers across most layers and for both batch sizes.

Although the approximation quality of \emph{Apollo} improves significantly with larger batch sizes, even approaching that of \emph{Adam} and \emph{AdaBelief},
it still fails to match their performance.
\emph{AdaHessian} is able to produces more accurate approximations than \emph{Adam} and \emph{AdaBelief} both in the small batch setting,
as well as in the large batch setting.
Finally, we see that \emph{AdaBelief} performs similarly to \emph{Adam}, but is able to provide a noticeably better approximation of the batch Hessian when less noisy gradient estimates are available.
This brings us to the conclusion that, although we could only evaluate \emph{Apollo} on a small model,
it does not seem to live up to its claim of providing a better curvature approximation than traditional first-order methods.
\emph{AdaHessian}, on the other hand, is able to significantly outperform both traditional first-order methods, as well as \emph{Apollo},
in terms of curvature approximation.
However, due to the nature of \emph{AdaHessian}'s stochastic approximation, it will always require a warm-up period for its approximation to become accurate,
as $ \text{diag}(H) = \mathbb{E}[z \odot (Hz)]$ (see \ref{sec:adahessian}) needs several evaluations of $z \odot (Hz)$ . We can see this in Figure \ref{fig:cosine-small-batch},
were it starts of with a much higher degree until arriving at better approximates in later steps.


The good approximation performance of \emph{AdaBelief},in the \emph{big} batch setting,  may be explained by regarding the \emph{EMA} of the belief term, $(g_t - m_t)^2$ (see \ref{sec:adabelief}), 
as a approximation for the diagonal entries of the gradient variance, as $Var(g)_{ii} = E[(g_i - E[g_i])^2] \quad g \in \mathbb{R}^n, i \leq n$.
It can be shown that if we model our loss function as the negative log-likelihood, $\mathcal{L}(y,x,\theta) = - \log p(y|x,\theta)$, with $y \in \mathbb{R}^m$ beeing the lables of the input $x\in \mathbb{R}^n$, then
\begin{equation}
    \text{Var}(\nabla \log p(y|x,\theta)) = \mathbb{E}[\nabla^2 \log p(y|x,\theta)] = - \mathbb{E}[H_{\log p(y|x,\theta)}] \quad \text{\cite{jaketae_fisher}}.
\end{equation}
In this way, the belief term of \emph{AdaBelief} directly approximates the diagonal entries of the expected negative Hessian.
This might provide a new perspective on the superior approximation abilities of \emph{AdaBelief},
as we have not seen this connection in the literature yet, to the best of our knowledge.
Finally, the most interesting observation arises when we examine Figure \ref{fig:loss-big-batch},
where the evolution of the loss is depicted for both batch sizes.
We can see that, despite tuning each optimizer for optimal performance, with Hyperparameter in \ref{tab:curve-approx-params}, \emph{AdaHessian} is unable to achieve the same loss as \emph{Apollo},
even though it is capable of much more accurate curvature approximations. \emph{Apollo} on the other hand is 
able to achive basicly the same convergence behavior as first-oder methods, although providing much less accurate curvature
approaximations.
This raises the question of whether very accurate curvature estimates might actually hinder
the model's performance. It is conceivable that such precision could lead to an
earlier discovery of local minima, which are suboptimal, whereas optimizers like
\emph{Apollo} may explore more of the loss surface while still accounting for curvature.
However, as we observed in the benchmarks on \emph{CIFAR-10} and \emph{WMT-14}, both \emph{AdaHessian} and \emph{Apollo} are able to discover minima that generalize well.
This leads to the hypothesis that the better generalization performance of the tested second-order
methods may not be solely based on superior Hessian approximation capabilities,
but rather on other mechanisms that have yet to be uncovered. In conclusion, our analysis shows that \emph{Apollo} is not competitive with standard first-order methods in terms of curvature approximation ability,
at least within the context of our small testing network. However, \emph{Apollo} still manages to outperform first-order optimizers with respect to generalization performance.
Given that the increase in resource consumption for \emph{Apollo} is relatively modest, it may be a suitable option when maximal test performance is prioritized over faster convergence.
Furthermore, since \emph{AdaHessian} requires 2 to 3 times the memory of \emph{SGD} (see Table \ref{tab:optimizer_comparison_perf})
and due to PyTorch's current inability to work with gradient graphs on \emph{DDP}, training large, distributed models becomes infeasible.
Therefore, one might conclude that the approximation of \emph{AdaHessian} is unlikely to find widespread application over existing first-order methods.
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-big-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{big} batch (1028 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-big-batch}.}
    \label{fig:cosine-big-batch}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-small-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (128 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-small-batch}.
}
    \label{fig:cosine-small-batch}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/loss.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The log loss of the model during training, y-axis, after each update step, x-axis, while training with  \emph{small}- (left) and \emph{big} batches (right) of training data.}
    \label{fig:loss-big-batch}
\end{figure}



\comment{
\section{Optimizer Behavior in Parameter Space}
To gain deeper insight into how different second-order approximations influence the movement of optimizers
through the parameter space, we will define several metrics whose evolution we will plot over the entire training
duration.
\subsection{Step Length}
To measure how the step length in the parameter space develops over time,
we will record $\alpha$, where $\alpha = \lVert \Delta \theta \rVert_2$ and $\Delta \theta = \theta_t - \theta_{t-1}$, $t \in \mathbb{N}$.
\subsection{Distance Traveled}
To measure the exploration ability of the parameter space by each optimizer,
we introduce $\beta$, where $\beta = \lVert \theta_{t} - \theta_{0} \rVert_2$, $t \in \mathbb{N}$.
This metric indicates how far the optimizer has traveled from its initial point to the
current point in time.
\subsection{Step Oscillations}
To measure how much the optimizers oscillate between steps, we measure the angle between two successive update steps. We define this angle $\gamma$ as
\[
    \gamma = \arccos\left(\frac{\langle \Delta \theta_t, \Delta \theta_{t-1} \rangle}{\lVert \Delta \theta_t \rVert \cdot \lVert \Delta \theta_{t-1} \rVert}\right),
\]
where 
\[
    \Delta \theta_{t} =  \theta_{t} -  \theta_{t-1}, \quad \Delta \theta_{t-1} =  \theta_{t-1} -  \theta_{t-2}.
\]
\subsection{Sharpness of Minima}
To measure the sharpness of the minima found by the optimizers during training, we focus on the curvature of these minima. 
Minima with flatter curvature are generally associated with better generalization performance because small changes in the input are less likely to significantly affect the output of the loss function.
To quantify this, we define the metric \( \phi \) as 
\[
    \phi =  \frac{1}{n}\sum_{i=0}^{n} \left| \frac{\partial^2 \mathcal{L}}{\partial \theta_{ii}} \right|.
\]
This metric, \( \phi \), provides a measure of the absolute mean curvature at a given point in the parameter space.
A lower value of \( \phi \) indicates flatter minima, which are typically associated with better generalization.
In contrast, a higher \( \phi \) value corresponds to sharper minima.
\subsection{Results}
}



\comment{\section{The SApollo Optimizer}
As we noted in the previous section, the approximation quality of the batch Hessian in \emph{Apollo} is not competitive with existing first-order methods.
However, an interesting new investigation could involve smoothing the calculated Hessian diagonal in \emph{Apollo}.
As we recall from Apollo's Algorithm \ref{alg:apollo}, $B_t$ represents the Hessian diagonal that it outputs.
Unlike other optimizers, \emph{Apollo} does not smooth these approximations of $B_t$ over multiple steps to reduce variance.
The authors themselves suggest that investigating the moving average of the diagonal $B_t$ might be a promising direction for future work \cite{apollo}.
In the following, we will explore this by applying an \emph{EMA} (Exponential Moving Average) to smooth $B_t$ and investigate the consequences on Hessian approximation quality and the converge of the loss.
We will call this \emph{SApollo} (\textbf{S}moothedApollo) \ref{alg:Sapollo}.

\marginpar{\cite{apollo}}
\label{alg:Sapollo}
\begin{algorithm}
    \caption{SApollo}
    \begin{algorithmic}[1]
    \State \textbf{Initial:} $m_0, d_0, B_0, v_0 \leftarrow 0, 0, 0,0$ \Comment{Initialize $m_0, d_0, B_0$ to zero}
    \State Good default settings are $\beta_1 = 0.9,\boldsymbol{\beta_2 = 0.999}$ and $\epsilon = 10^{-4}$
    \While{$t \in \{0, \ldots, T\}$}
        \For{$\theta \in \{\theta_1, \ldots, \theta_L\}$}
            \State $g_{t+1} \leftarrow \nabla f_t(\theta_t)$ \Comment{Calculate gradient at step $t$}
            \State $m_{t+1} \leftarrow \frac{\beta_1(1-\beta_1^t)}{1-\beta_1^{t+1}}m_t + \frac{1-\beta_1}{1-\beta_1^{t+1}}g_{t+1}$ \Comment{Bias-corrected EMA}
            \State $\alpha \leftarrow \frac{d_t^T(m_{t+1}-m_t)+d_t^TB_td_t}{(\|d_t\|_4+\epsilon)^4}$ \Comment{Calculate coefficient of $B$ update}
            \State $B_{t+1} \leftarrow B_t - \alpha \cdot \text{Diag}(d_t^2)B_{t+1} $ \Comment{Update diagonal Hessian}
            \State $\boldsymbol{v_{t+1} \leftarrow \beta_2 v_t + (1-\beta_2)B_{t+1}}$ \Comment{\textbf{Update EMA of B}}
            \State $\boldsymbol{\hat{v}_{t+1} \leftarrow \frac{v_t}{1-\beta_2^{t+1}}}$ \Comment{\textbf{Bias-corrected EMA}}

            \State $D_{t+1} \leftarrow \text{rectify}(\boldsymbol{\hat{v}_{t+1}}, 0.01)$ \Comment{Handle nonconvexity}
            \State $d_{t+1} \leftarrow D_{t+1}^{-1}m_{t+1}$ \Comment{Calculate update direction}
            \State $\theta_{t+1} \leftarrow \theta_t - \eta_{t+1}d_{t+1}$ \Comment{Update parameters}
        \EndFor
    \EndWhile
    \State \textbf{return} $\theta_T$
    \end{algorithmic}
    \end{algorithm}

\section{Optimizer Behaviour Visualization}

\section{Hessian Diagonal in FFN's}
}
