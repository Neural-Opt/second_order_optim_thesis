%************************************************
\chapter{The Hessian in Neural Networks}\label{ch:mathtest} % $\mathbb{ZNR}$
%************************************************
In this chapter, we will closely examine the behavior of \emph{Apollo} and \emph{AdaHessian} to understand how they differ from established first-order methods and whether their resource overhead is justified.
As both \emph{Apollo} and \emph{AdaHessian} claim to provide more accurate second-order estimates
than their predecessors, we therefore compare the calculated batch Hessian
diagonal with the approximations generated by \emph{Apollo}, \emph{AdaHessian}, \emph{Adam}, and \emph{AdaBelief}
across different batch sizes.

\section{Hessian Approximization Quality}
Before comparing the quality of the Hessian approximations provided by the previously mentioned optimizers,
 we need to address some additional details.
Recalling their definitions, \emph{Adam}, \emph{AdaBelief}, and \emph{AdaHessian} all use an estimate of the
absolute curvature in their second-moment computations.
The diagonal elements of the Hessian matrix of the loss function consist of the second-order partial derivatives with respect to each parameter $H_{ii} = \frac{\partial^2 L}{\partial \theta_i^2},$
where \( L \) is the loss function and \( \theta_i \) is the \( i \)-th parameter.
The entries \( H_{ii} \) are negative when the loss function is locally concave with respect to \( \theta_i \) and positive when it is locally convex.
Consequently, we will compare the \emph{absolute} values of the Hessian diagonal elements with the optimizers' approximations,
 since they only consider the \emph{absolute} curvatureâ€”that is, they ignore the sign of the Hessian diagonal entries, 
 as they all use the squared approaximations in their second moment.
To do so, we first calculate the Hessian diagonal for the current batch using \texttt{torch.autograd}.
Since PyTorch does not natively support the calculation of the Hessian diagonal in a fully vectorized form,
we implemented this by iterating over each gradient element, performing a second backward pass, and extracting the corresponding second-order derivative.
Given that this process is inherently slow, we limited our investigation to a relatively small CNN model with 13.5K parameters, consisting of two convolutional layers and two fully connected layers.
For training, we used the MNIST dataset, which contains 60,000 images of handwritten digits, each of size 32 $\times$ 32.
To measure the similarity between the approximated Hessian diagonal and the actual batch Hessian diagonal, we needed a metric that is independent of the magnitude of the vectors.
This is crucial because the scale of the second moments can be adjusted through the learning rate.
Therefore, we utilized the cosine similarity measure, which is defined as follows:
\[
\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}, \quad \mathbf{A}, \mathbf{B} \in \mathbb{R}^n.
\]
This provides a measure of the angle between the two vectors, where \( \cos(\theta) = 1 \) indicates perfect directional alignment, and \( \cos(\theta) = 0 \) indicates orthogonality.
We record the measured cosine similarity and plot its development throughout the training process.
This is done for both batch sizes of 128 and 1024 to observe whether the increased stochastic variance in the gradient significantly impacts the quality of the approximation results.
In Figures \ref{fig:cosine-small-batch} and \ref{fig:cosine-big-batch}, we observe the angle between the approximations
and the \emph{absolute} batch Hessian diagonal for both small and large batch sizes.
For improved visualization, a moving average with a window size of 10 is applied to the small batch plots.
The results indicate that \emph{Apollo}'s approximations are much worse than those of the other optimizers across most layers and for both batch sizes.

Although the approximation quality of \emph{Apollo} improves significantly with larger batch sizes, even approaching that of \emph{Adam} and \emph{AdaBelief},
it still fails to match their performance.
\emph{AdaHessian} is able to produces more accurate approximations than \emph{Adam} and \emph{AdaBelief} both in the small batch setting,
as well as in the large batch setting.
Finally, we see that \emph{AdaBelief} performs similarly to \emph{Adam}, but is able to provide a noticeably better approximation of the batch Hessian when less noisy gradient estimates are available.
This brings us to the conclusion that, although we could only evaluate \emph{Apollo} on a small model,
it does not seem to live up to its claim of providing a better curvature approximation than traditional first-order methods.
\emph{AdaHessian}, on the other hand, is able to significantly outperform both traditional first-order methods, as well as \emph{Apollo},
in terms of curvature approximation.
However, due to the nature of \emph{AdaHessian}'s stochastic approximation, it will always require a warm-up period for its approximation to become accurate,
as $ \text{diag}(H) = \mathbb{E}[z \odot (Hz)]$ (see \ref{sec:adahessian}) needs several evaluations of $z \odot (Hz)$ . We can see this in Figure \ref{fig:cosine-small-batch},
were it starts of with a much higher degree until arriving at better approximates in later steps.


The good approximation performance of \emph{AdaBelief},in the \emph{big} batch setting,  may be explained by regarding the \emph{EMA} of the belief term, $(g_t - m_t)^2$ (see \ref{sec:adabelief}), 
as a approximation for the diagonal entries of the gradient variance, as $Var(g)_{ii} = E[(g_i - E[g_i])^2] \quad g \in \mathbb{R}^n, i \leq n$.
It can be shown that if we model our loss function as the negative log-likelihood, $\mathcal{L}(y,x,\theta) = - \log p(y|x,\theta)$, with $y \in \mathbb{R}^m$ beeing the lables of the input $x\in \mathbb{R}^n$, then
\begin{equation}
    \text{Var}(\nabla \log p(y|x,\theta)) = \mathbb{E}[\nabla^2 \log p(y|x,\theta)] = - \mathbb{E}[H_{\log p(y|x,\theta)}] \quad \text{\cite{jaketae_fisher}}.
\end{equation}
In this way, the belief term of \emph{AdaBelief} directly approximates the diagonal entries of the expected negative Hessian.
This might provide a new perspective on the superior approximation abilities of \emph{AdaBelief},
as we have not seen this connection in the literature yet, to the best of our knowledge.
Finally, the most interesting observation arises when we examine Figure \ref{fig:loss-big-batch},
where the evolution of the loss is depicted for both batch sizes.
We can see that, despite tuning each optimizer for optimal performance, with Hyperparameter in \ref{tab:curve-approx-params}, \emph{AdaHessian} is unable to achieve the same loss as \emph{Apollo},
even though it is capable of much more accurate curvature approximations. \emph{Apollo} on the other hand is 
able to achive basicly the same convergence behavior as first-oder methods, although providing much less accurate curvature
approaximations.
This raises the question of whether very accurate curvature estimates might actually hinder
the model's performance. It is conceivable that such precision could lead to an
earlier discovery of local minima, which are suboptimal, whereas optimizers like
\emph{Apollo} may explore more of the loss surface while still accounting for curvature.
However, as we observed in the benchmarks on \emph{CIFAR-10} and \emph{WMT-14}, both \emph{AdaHessian} and \emph{Apollo} are able to discover minima that generalize well.
This leads to the hypothesis that the better generalization performance of the tested second-order
methods may not be solely based on superior Hessian approximation capabilities,
but rather on other mechanisms that have yet to be uncovered.
We will undermine this oberservation by uncovering several flaws in \emph{Apollo}, both theoretical and practical,
including issues related to its implementation in the next section.
In conclusion, our analysis shows that \emph{Apollo} is not competitive with standard first-order methods in terms of curvature approximation ability,
at least within the context of our small testing network.
Furthermore, since \emph{AdaHessian} requires 2 to 3 times the memory of \emph{SGD} (see Table \ref{tab:optimizer_comparison_perf})
and due to PyTorch's current inability to work with gradient graphs on \emph{DDP}, training large, distributed models becomes infeasible.
Therefore, one might conclude that the approximation of \emph{AdaHessian} is unlikely to find widespread application over existing first-order methods.
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-big-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{big} batch (1028 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-big-batch}.}
    \label{fig:cosine-big-batch}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-small-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (128 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-small-batch}.
}
    \label{fig:cosine-small-batch}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/loss.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The log loss of the model during training, y-axis, after each update step, x-axis, while training with  \emph{small}- (left) and \emph{big} batches (right) of training data.}
    \label{fig:loss-big-batch}
\end{figure}

\comment{
    \begin{python}
        (...)
        bias_correction = 1 - beta ** state['step']
        alpha = (1 - beta) / bias_correction
    
        # calc the diff grad
        delta_grad = grad - exp_avg_grad
        rebound = 0.01
        eps = eps / rebound
    
        
        # Update the running average grad
        exp_avg_grad.add_(delta_grad, alpha=alpha)
        denom = d_p.norm(p=4).add(eps)
        d_p.div_(denom)
        v_sq = d_p.mul(d_p)
        delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-alpha) - B.mul(v_sq).sum()
        # Update B
        B.addcmul_(v_sq, delta)
        # calc direction of parameter updates
        denom = B.abs().clamp_(min=rebound)
        d_p.copy_(exp_avg_grad.div(denom))
    
        # Perform step weight decay
        (...)
        p.add_(d_p, alpha=-curr_lr)
        \end{python}
           
}
 
\section{The SApollo Optimizer}
As we noted in the previous section, the approximation quality of the batch Hessian in \emph{Apollo} is not competitive with existing first-order methods.
In this section, we will examine several flaws arising from the formulation and implementation of the \emph{Apollo} algorithm (see Algorithm \ref{alg:apollo}).
We will then address both theoretical and practical issues, aiming to resolve these errors in the algorithm's design and implementation.
We begin by demonstrating that \textbf{Theorem 1} in \emph{Apollo} \cite{apollo} does not hold,
which may cause the algorithm to behave similarly to \emph{SGD} with momentum, thereby explaining its poor curvature approximation abilities.\\

\noindent\textbf{Theorem 1 (Coupled Stepsize and Convexity (from \cite{apollo})).} 
\textit{Given zero initialization of $m_0$, $d_0$, and $B_0$ and a fixed parameter intialization
$\theta_0$. Suppose that we have two sets of hyper-parameters $\eta$, $\sigma$ and $\eta'$, $\sigma'$ with the same ratio:
$\frac{\eta}{\sigma} = \frac{\eta'}{\sigma'}$. Then the convergence trajectories of these two sets of hyper-parameters are exactly
the same:
        \[ \theta_{t} = \theta'_{t}, \quad t \in \{1,\cdots,T \}\]
}
The paper adopts an inductive approach for this proof, where they define the invariant as 
\begin{equation}
    m_t = m'_t, \quad B_t = \alpha B'_t, \quad \theta_t = \theta'_t$, with $\alpha = \frac{\eta'}{\eta} = \frac{\sigma'}{\sigma} \quad \forall t.
\end{equation}
As the error lies in the formulation of the inital step of the induction, we will refer to the paper\cite{apollo} for the inductive step. 
They formulate the  inital step  as follows:
When $t = 1$, since $\theta_0 = \theta'_0$, we have $m_1 = m'_1$. With $d_0 = d'_0 = 0$, we have $B_1 = B'_1 = 0$, and:
\[
D_1 = \text{rectify}(B_1, \sigma) = \sigma
\]
\[
D'_1 = \text{rectify}(B'_1, \sigma') = \sigma'
\]
Thus, $D'_1 = \alpha D_1$, and:
\[
\theta'_1 = \theta'_0 - \eta' D'^{-1}_1 m'_1 = \theta_0 - \eta \alpha (D_1^{-1} / \alpha) m_1 = \theta_0 - \eta D_1^{-1} m_1 = \theta_1.
\]
However, as seen from the formulation of the $B$ update in Algorithm \ref{alg:apollo}, $B_1 = B'_1$ only holds if $m_1 = 0$,
which is generally not the case. Consequently, $D'_1 \neq \alpha D_1$, and therefore $\theta'_1 \neq \theta_1$.
This implies that the clamping effect, due to the choice of $\sigma$ in \emph{Apollo}, cannot be rescaled by adjusting the learning rate. 
This is expected, as the clamping operation is inherently non-linear.
In practice, this results in small values being capped by $\sigma$, which effectively leads to a rescaling of the learning rate by a 
factor of $\frac{1}{\sigma}$, disregarding any curvature information at that point. Thus, the higher the choice of $\sigma$,
the more similar \emph{Apollo}'s update step becomes to plain \emph{SGD} with momentum.
This phenomenon occurred in Figure \ref{fig:cosine-bias-big-batch}, where most values of $B$ were much smaller than the chosen $\sigma$,
leading to a loss of curvature information.





