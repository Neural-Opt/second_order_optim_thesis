%************************************************
\chapter{Hessian Approximization Quality and SApollo}\label{ch:mathtest} % $\mathbb{ZNR}$
%************************************************
In this chapter, we will closely examine the behavior of \emph{Apollo} and \emph{AdaHessian} to understand how they differ from established first-order methods and whether their resource overhead is justified.
As both \emph{Apollo} and \emph{AdaHessian} claim to provide more accurate second-order estimates
than their predecessors, we therefore compare the calculated batch Hessian
diagonal with the approximations generated by \emph{Apollo}, \emph{AdaHessian}, \emph{Adam}, and \emph{AdaBelief}
across different batch sizes.

\section{Hessian Approximization Quality}
Before comparing the quality of the Hessian approximations provided by the previously mentioned optimizers,
 we need to address some additional details.
Recalling their definitions, \emph{Adam}, \emph{AdaBelief}, and \emph{AdaHessian} all use an estimate of the
absolute curvature in their second-moment computations.
The diagonal elements of the Hessian matrix of the loss function consist of the second-order partial derivatives with respect to each parameter $H_{ii} = \frac{\partial^2 L}{\partial \theta_i^2},$
where \( L \) is the loss function and \( \theta_i \) is the \( i \)-th parameter.
The entries \( H_{ii} \) are negative when the loss function is locally concave with respect to \( \theta_i \) and positive when it is locally convex.
Consequently, we will compare the \emph{absolute} values of the Hessian diagonal elements with the optimizers' approximations,
 since they only consider the \emph{absolute} curvatureâ€”that is, they ignore the sign of the Hessian diagonal entries, 
 as they all use the squared approaximations in their second moment.
To do so, we first calculate the Hessian diagonal for the current batch using \texttt{torch.autograd}.
Since PyTorch does not natively support the calculation of the Hessian diagonal in a fully vectorized form,
we implemented this by iterating over each gradient element, performing a second backward pass, and extracting the corresponding second-order derivative.
Given that this process is inherently slow, we limited our investigation to a relatively small CNN model with 13.5K parameters, consisting of two convolutional layers and two fully connected layers.
For training, we used the MNIST dataset, which contains 60,000 images of handwritten digits, each of size 28 $\times$ 28.
To measure the similarity between the approximated Hessian diagonal and the actual batch Hessian diagonal, we needed a metric that is independent of the magnitude of the vectors.
This is crucial because the scale of the second moments can be adjusted through the learning rate.
Therefore, we utilized the cosine similarity measure, which is defined as follows:
\[
\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}, \quad \mathbf{A}, \mathbf{B} \in \mathbb{R}^n.
\]
This provides a measure of the angle between the two vectors, where \( \cos(\theta) = 1 \) indicates perfect directional alignment, and \( \cos(\theta) = 0 \) indicates orthogonality.
We record the measured cosine similarity and plot its development throughout the training process.
This is done for both batch sizes of 124 and 1024 to observe whether the increased stochastic variance in the gradient significantly impacts the quality of the approximation results.
In Figures \ref{fig:cosine-small-batch} and \ref{fig:cosine-big-batch}, we observe the angle between the approximations
and the \emph{absolute} batch Hessian diagonal for both small and large batch sizes.
For improved visualization, a moving average with a window size of 10 is applied to the small batch plots.
The results indicate that \emph{Apollo}'s approximations are much worse than those of the other optimizers across most layers and for both batch sizes.

Although the approximation quality of \emph{Apollo} improves significantly with larger batch sizes, even approaching that of \emph{Adam} and \emph{AdaBelief},
it still fails to match their performance.
\emph{AdaHessian} is able to produces more accurate approximations than \emph{Adam} and \emph{AdaBelief} both in the small batch setting,
as well as in the large batch setting.
Finally, we see that \emph{AdaBelief} performs similarly to \emph{Adam}, but is able to provide a noticeably better approximation of the batch Hessian when less noisy gradient estimates are available.
This brings us to the conclusion that, although we could only evaluate \emph{Apollo} on a small model,
it does not seem to live up to its claim of providing a better curvature approximation than traditional first-order methods.
\emph{AdaHessian}, on the other hand, is able to significantly outperform both traditional first-order methods, as well as \emph{Apollo},
in terms of curvature approximation.
However, due to the nature of \emph{AdaHessian}'s stochastic approximation, it will always require a warm-up period for its approximation to become accurate,
as $ \text{diag}(H) = \mathbb{E}[z \odot (Hz)]$ (see \ref{sec:adahessian}) needs several evaluations of $z \odot (Hz)$ . We can see this in Figure \ref{fig:cosine-small-batch},
were it starts of with a much higher degree until arriving at better approximates in later steps.


The good approximation performance of \emph{AdaBelief},in the \emph{big} batch setting,  may be explained by regarding the \emph{EMA} of the belief term, $(g_t - m_t)^2$ (see \ref{sec:adabelief}), 
as a approximation for the diagonal entries of the gradient variance, as $Var(g)_{ii} = E[(g_i - E[g_i])^2] \quad g \in \mathbb{R}^n, i \leq n$.
It can be shown that if we model our loss function as the negative log-likelihood, $\mathcal{L}(y,x,\theta) = - \log p(y|x,\theta)$, with $y \in \mathbb{R}^m$ beeing the lables of the input $x\in \mathbb{R}^n$, then
\begin{equation}
    \text{Var}(\nabla \log p(y|x,\theta)) = \mathbb{E}[\nabla^2 \log p(y|x,\theta)] = - \mathbb{E}[H_{\log p(y|x,\theta)}] \quad \text{\cite{jaketae_fisher}}.
\end{equation}
In this way, the belief term of \emph{AdaBelief} directly approximates the diagonal entries of the expected negative Hessian.
This might provide a new perspective on the superior approximation abilities of \emph{AdaBelief},
as we have not seen this connection in the literature yet, to the best of our knowledge.
Finally, the most interesting observation arises when we examine Figure \ref{fig:loss-big-batch},
where the evolution of the loss is depicted for both batch sizes.
We can see that, despite tuning each optimizer for optimal performance, with Hyperparameter in \ref{tab:curve-approx-params}, \emph{AdaHessian} is unable to achieve the same loss as \emph{Apollo},
even though it is capable of much more accurate curvature approximations. \emph{Apollo} on the other hand is 
able to achive basicly the same convergence behavior as first-oder methods, although providing much less accurate curvature
approaximations.
Additionally, as we observed in the benchmarks on \emph{CIFAR-10} and \emph{WMT-14}, both \emph{AdaHessian} and \emph{Apollo} are able to discover minima that generalize well.
This leads to the hypothesis that the better generalization performance of the tested second-order
methods may not be solely based on superior Hessian approximation capabilities,
but rather on other mechanisms that have yet to be uncovered.
We will undermine this oberservation by uncovering several flaws in \emph{Apollo}, related to its implementation in the next section.
In conclusion, our analysis shows that \emph{Apollo} is not competitive with standard first-order methods in terms of curvature approximation ability,
at least within the context of our small testing network.
Furthermore, since \emph{AdaHessian} requires 2 to 3 times the memory of \emph{SGD} (see Table \ref{tab:optimizer_comparison_perf})
and due to PyTorch's current inability to work with gradient graphs on \emph{DDP}, training large, distributed models becomes infeasible.
Therefore, one might conclude that the approximation of \emph{AdaHessian} is unlikely to find widespread application over existing first-order methods.
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-big-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{big} batch (1028 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-big-batch}.}
    \label{fig:cosine-big-batch}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/hess-approx-graph-small-batch.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (124 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{weights}. For the corresponding analysis on biases, please refer to Figure \ref{fig:cosine-bias-small-batch}.
}
    \label{fig:cosine-small-batch}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/loss.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The log loss of the model during training, y-axis, after each update step, x-axis, while training with  \emph{small}- (left) and \emph{big} batches (right) of training data.}
    \label{fig:loss-big-batch}
\end{figure}

 
\section{The SApollo Optimizer}
As we noted in the previous section, the approximation quality of the batch Hessian in \emph{Apollo} is not competitive with existing first-order methods.
In this section, we will examine several flaws arising from the formulation and implementation of the \emph{Apollo} algorithm (see Algorithm \ref{alg:apollo}).
We will then address both theoretical and practical issues, aiming to resolve these errors in the algorithm's design and implementation and comparing
the results.
We begin by demonstrating that the clamping operation in \emph{Apollo} can lead to curvature information beeing lost,
which causes the algorithm to behave similarly to \emph{SGD} with momentum, thereby explaining its poor curvature approximation abilities.\\
For this we look at line \textbf{9} in \ref{alg:apollo}. Here we can see that all values of $B_t$ with $|B_t| \le \sigma$ are clamped,
losing their curvature information. 

This implies that the clamping effect, due to the choice of $\sigma$ in \emph{Apollo}, cannot be rescaled by adjusting the learning rate. 
This is expected, as the clamping operation is inherently non-linear.
In practice, this results in small values being capped by $\sigma$, which effectively leads to a rescaling of the learning rate by a 
factor of $\frac{1}{\sigma}$, disregarding any curvature information at that point. Thus, the higher the choice of $\sigma$,
the more similar \emph{Apollo}'s update step becomes to plain \emph{SGD} with momentum.
This phenomenon occurred in Figure \ref{fig:cosine-big-batch}, where most values of $B$ were much smaller than the chosen $\sigma$,
leading to a loss of curvature information.
Note that in the standard implementation of \emph{Apollo}, $\sigma$ cannot be choosen as a hyperparamter, as it is hardcoded in the optimizer.
To gain intuition for the small values of $B$ from a theoretical perspective, we will again formulate the update rule of $B$, with
\begin{align}
    &B_{t+1} =  B_t - \frac{d_t^T (\Delta m_{t+1}) - (d_t^2)^T B_t}{\left( \|d_t\|_4 + \epsilon \right)^4} \cdot d_t^2 \\
    &= B_t - \frac{\sum^n_j{d_{t,j}\Delta m_{t+1,j} - \sum^n_j{d_{t,j}^2 B_{t,j} }}}{\left( \|d_t\|_4 + \epsilon \right)^4} \cdot d_t^2.
\end{align}
We now substitute $d_t = \frac{m_t}{\gamma}, \quad \gamma = \max(|B_t|, \sigma)$, and obtain the following for the $k$-th entry of $B_t$
\begin{align*}
    \Delta B_{t+1,k} &=\left(-\displaystyle\sum^n_j \frac{m_{t,j}}{\gamma_j } \Delta m_{t+1,j} - \displaystyle\sum^n_j \frac{m_{t,j}^2}{\gamma_j ^2} B_{t,j}\right) \cdot \frac{m_{t,k}^2}{\gamma_k^2} \left( \|\frac{m_t}{\gamma}\|_4 + \epsilon \right)^{-4}  \\
    &= \left(\displaystyle\sum^n_j -\frac{m_{t,j}\Delta m_{t+1,j}m_{t,k}^2}{\gamma_j \gamma_k^2} - \frac{m_{t,j}^2m_{t,k}^2}{\gamma_j ^2\gamma_k^2} B_{t,j}\right) \left( \|\frac{m_t}{\gamma}\|_4 + \epsilon \right)^{-4}  \\
    &\approx \left(\displaystyle\sum^n_j -\frac{m_{t,j}\Delta m_{t+1,j}m_{t,k}^2}{\gamma_j \gamma_k^2} - \frac{m_{t,j}^2m_{t,k}^2}{\gamma_j ^2\gamma_k^2} B_{t,j}\right) \left( \|\frac{m_t}{\gamma}\|_4\right)^{-4}  \\
    &=  \Delta \hat{B}_{t+1,k}.
\end{align*}
We will now examine the state of the algorithm in a situation where $B_t$ is small, to observe how the updates behave in such a case.
Specifically, we consider $B_{t,k} \leq \sigma$, and thus set $\gamma = \sigma$, which gives us the following expression

\begin{align*}
    \Delta \hat{B}_{t+1,k} &= \left(\sum_{j=1}^{n} -\frac{m_{t,j} \Delta m_{t+1,j} m_{t,k}^2}{\sigma^3}  - \frac{m_{t,j}^2m_{t,k}^2}{\sigma^4} B_{t,j}  \right) \sigma^{-4} \left(\sum_{j=1}^{n} m_{t,j}^{4} \right)^{-1}\\
    &=  \left(\sum_{j=1}^{n} -\sigma m_{t,j} \Delta m_{t+1,j} m_{t,k}^2  - m_{t,j}^2m_{t,k}^2 B_{t,j} \right)  \left(\sum_{j=1}^{n} m_{t,j}^{4} \right)^{-1}.
\end{align*}
As $\Delta m_{t+1}$ is typically smaller than the gradients themselves, and when $m_{t,k} \ll m_{t,j}$, 
the updates to $B$ can become very small. Due to the stochastic nature of the gradients,
where the sign of updates may fluctuate from batch to batch, $B$ is unable to accumulate significant values.
Consequently, the values of $B$ remain small and are clamped to $\sigma$ before gradient preconditioning,
resulting in a loss of crucial curvature information.


As mentioned earlier, there are also multiple issues in the implementation of \emph{Apollo} (see \ref{fig:sapollo_imp}),
which deviate from its theoretical formulation. In line \textbf{5}, we see the implementation of $\Delta m_{t+1}$.
However, \texttt{delta\_grad} is not the true difference between the current and previous gradient,
but rather the difference between the current gradient and \texttt{exp\_avg\_grad}, where \texttt{exp\_avg\_grad}
is not the EMA of the gradient, but a recursive form of $\texttt{exp\_avg\_grad}_{t+1} = (1-\beta) (\texttt{grad} - \texttt{exp\_avg\_grad}_{t})$ (see line \textbf{9}).
Additionally, this value is used as a replacement for the EMA of the gradient in line \textbf{19},
where it is preconditioned using the Hessian diagonal approximation.
Furthermore, the bias correction in line \textbf{9} does not follow the standard approach for updating an EMA (see Section \ref{sec:adam}).
We suspect that the authors may have incorrectly updated \texttt{exp\_avg\_grad} with \texttt{delta\_grad} instead of \texttt{grad},
as doing so would align with the theoretical formulation, aside from the incorrect bias correction.

    To address the identified issues, we propose a modified version of the Apollo optimizer, named \emph{SApollo (Smoothed Apollo)}, which adheres to its theoretical formulation. Additionally, we incorporate an Exponential Moving Average (EMA) of the Hessian diagonal approximations, denoted as $B$, as suggested by the authors of \emph{Apollo} as an interesting direction for future research \cite{apollo}.
    Figure \ref{fig:sapollo_imp} illustrates our implementation. We calculate \texttt{exp\_avg\_grad} as a properly implemented EMA of the gradient and use it in the preconditioning step.
    Furthermore, we calculate \texttt{delta\_grad}, by capturing the difference between the current gradient and it's EMA, similar to AdaBelief (see \ref{sec:adabelief}).
    The Hessian diagonal approximation $B$ is implemented as a moving average of the updates derived from Eq. \ref{eq:apollo-update}.
    Note that this does not require additional memory, as we simply replace the non-EMA version with the EMA version of $B$.
    As we identified the clamping operation to be problematic for small values, we instead add $\sigma$ to the absolute values of $B$ (line \textbf{21}) if they are smaller than $\sigma$.
    This means we are using $\sigma$ as a dampening factor when values of $B$ are very small.
    Although this can lead to a learning rate decrease of at most twice the amount of \emph{Apollo}, it is able to capture
    important curvature information. To tune $\sigma$ in practice, \emph{SApollo} introduces $\sigma$ as a hyperparameter, contrary to Apollo,
    where $\sigma$ is hardcoded in the optimizer.
    As we observed that the updates of $B$ can become very small, a high $\sigma$ may therefore lead to poor Hessian approximations.
    In our performance evaluation, we use $\sigma = 0.0001$ (lr=0.001), while for the task of Hessian approximation, we use $\sigma = 0.01$ for optimal comparability with \emph{Apollo}.
    It still needs to be determined how low the value of $\sigma$ should optimally be in practice.
    As this might vary across different model architectures, we believe introducing $\sigma$ as a hyperparameter is a good choice.
    Figure \ref{fig:sapollo-approx-weights} shows the results of the Hessian approximation. Note that these results only show the
    approximation quality regarding the \emph{weights}, for the \emph{biases} we refer to \ref{fig:sappolo-cosine-bias-small-batch}.
    While \emph{SApollo} is not yet competitive with first-order methods, it is on par or performs slightly better than \emph{Apollo} especially in later layers.
    As said earlier, we hypothesize that decreasing the value of $\sigma$ could further improve the quality of the Hessian approximation, as $\sigma=0.01$ may be too large,
    making it an interesting direction for future research.
    In Figure \ref{fig:sapollo-perf}, we present the performance comparison between \emph{SApollo} and  \emph{Apollo(W)}.
    The figure illustrates that \emph{SApollo} decreases the loss much faster than both \emph{ApolloW} and \emph{Apollo}.
    Although \emph{SApollo} performs better with $\sigma = 0.0001$ on this dataset, it remains to be determined whether this improvement can solely
    be attributed to the correction of the mentioned implementation mistakes,
    or if \emph{Apollo(W)} would perform on par with or better than \emph{SApollo} when its $\sigma$ is set to similar values.
    Furthermore, as \emph{Apollo} \cite{apollo} states, the reason for not introducing $\sigma$ as a hyperparameter is that the learning rate and $\sigma$ are coupled.
    This means that if the ratio between the learning rate and $\sigma$ remains the same across runs, the optimization trajectory shall also be the same (see Theorem 1 \cite{apollo}). 
    To test this in practice,
    we chose \texttt{lr}$=10^{-3}$ and $\sigma=10^{-4}$ for \emph{SApolloW} and applied the same ratio of \texttt{lr} and $\sigma$ to \emph{Apollo(W)}. Since $\frac{10^{-3}}{10^{-4}}=10$, we set \texttt{lr}$=0.1$ for \emph{Apollo(W)},
    as $\frac{10^{-3}}{10^{-4}}=10=\frac{10^{-1}}{10^{-2}}$ (see \ref{tab:sapollo-comp-params}). In our plot, we refer to these results as \emph{Apollo(W)-H-LR}.
    As we can see, the performance of \emph{Apollo(W)-H-LR} is significantly worse when compared to the other versions of the optimizer. 
    This indicates that the coupling of \texttt{lr} and $\sigma$ does not hold in practice, as the optimization trajectory between \emph{Apollo(W)} and \emph{Apollo(W)-H-LR} is \emph{not} the same,
    emphasizing the need to introduce $\sigma$ as a hyperparameter.

    Regarding memory optimization, since \texttt{d\_p} solely relies on values that are already stored, one could consider recomputing it at the start of each iteration
    using the not yet updated values of \texttt{exp\_avg\_grad} and $B$.
    This would reduce \emph{(S)Apollo}'s memory requirements to those of popular first-order methods, such as \emph{Adam}, which would 
    make it interesting for future investigation.

    \FloatBarrier
    \begin{figure}[H]
     \begin{minted}[linenos,frame=lines, framesep=2mm,breaklines, breakanywhere]{python}
    (...)
    bias_correction1 = 1 - beta1 ** state['step']
    bias_correction2 = 1 - beta2 ** state['step']

    # calc the diff grad
    delta_grad = grad - (exp_avg_grad/bias_correction1)
    sigma = 0.01
    # Update the running average grad
    exp_avg_grad.mul_(beta1).add_(grad, alpha=1 - beta1)

    denom = d_p.norm(p=4).add(eps)
    d_p.div_(denom)
    v_sq = d_p.mul(d_p)
    B_hat = B / bias_correction2
    delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-1) 
    - B_hat.mul(v_sq).sum()
    # Update B
    B.mul_(beta2).addcmul_(v_sq, delta, value=1 - beta2)
    B_hat = B / bias_correction2
    # calc direction of parameter updates
    denom = B_hat.abs()
    denom = torch.where(denom,denom < sigma,denom.add_(sigma),denom)
    d_p.copy_((exp_avg_grad/bias_correction1).div(denom))
    # Perform step weight decay
    (...)
    p.add_(d_p, alpha=-curr_lr)
    \end{minted}
    \begin{minted}[linenos,frame=lines, framesep=2mm,breaklines, breakanywhere]{python}
        (...)
        bias_correction = 1 - beta ** state['step']
        alpha = (1 - beta) / bias_correction
        # calc the diff grad
        delta_grad = grad - exp_avg_grad
        rebound = 0.01
        eps = eps / rebound
        # Update the running average grad
        exp_avg_grad.add_(delta_grad, alpha=alpha)
        denom = d_p.norm(p=4).add(eps)
        d_p.div_(denom)
        v_sq = d_p.mul(d_p)
        delta = delta_grad.div_(denom).mul_(d_p).sum().mul(-alpha) 
        - B.mul(v_sq).sum()
        # Update B
        B.addcmul_(v_sq, delta)
        # calc direction of parameter updates
        denom = B.abs().clamp_(min=rebound)
        d_p.copy_(exp_avg_grad.div(denom))
        # Perform step weight decay
        (...)
        p.add_(d_p, alpha=-curr_lr)
        \end{minted}

    
        \caption{The implementation of \emph{SApollo}(top) and  \emph{Apollo}(bottom) in PyTorch}
        \label{fig:sapollo_imp}
    
        \end{figure}
        \FloatBarrier
    
        \begin{figure}[h!]
            \centering
         \begin{minipage}[b]{\textwidth}
                \centering
                \begin{tabular}{cc}
                    \input{assets/chap3/sapollo-approx-weights.tex} % Pfad zu deiner .tex-Datei
                \end{tabular}
                \caption{
                    The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (124 samples).
                    Optimizer updates are denoted on the x-axis.
                }
                \label{fig:sapollo-approx-weights}
            \end{minipage}
            \vfill
            \begin{minipage}[b]{\textwidth}
                \centering
                \begin{tabular}{cc}
                    \input{assets/chap3/sapollo-perf.tex} % Pfad zu deiner .tex-Datei
                \end{tabular}
                \caption{
                    Evaluation of \emph{SApollo} on CIFAR-10 using ResNet-110 with the \emph{cosine annealing} learning rate scheduler.
                }
                \label{fig:sapollo-perf}
            \end{minipage}
        \end{figure}
        