%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Appendix}

%Errem omnium ea per, pro congue populo ornatus cu, ex qui dicant
%nemore melius. No pri diam iriure euismod. Graecis eleifend
%appellantur quo id. Id corpora inimicus nam, facer nonummy ne pro,
%kasd repudiandae ei mei. Mea menandri mediocrem dissentiet cu, ex
%nominati imperdiet nec, sea odio duis vocent ei. Tempor everti
%appareat cu ius, ridens audiam an qui, aliquid admodum conceptam ne
%qui. Vis ea melius nostrum, mel alienum euripidis eu.

\section{Appendix}
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap2/cifar10/cosine-real-comp.tex} \\ 
        
    \end{tabular}
    \caption{Evaluation of optimizers on CIFAR-10 using ResNet-110 with the \emph{cosine annealing} learning rate scheduler, where hyperparameters
    are held constant across all optimizers.For better visualization we applied a polynomial transformation, with $\hat{x}=x^\alpha$ and $\alpha=5$, for every $x \in \mathcal{D}$ in the output data $ \mathcal{D}$.}
    \label{fig:cifar-10-cosine-real}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/bias-sim-small.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (124 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{biases}.}
    \label{fig:cosine-bias-small-batch}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/bias-sim-big.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (124 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{biases}.}
    \label{fig:cosine-bias-big-batch}
\end{figure}
\begin{figure}[h!]
    \centering
    \begin{tabular}{cc}
        \input{assets/chap3/sapollo-approx-bias.tex} \\ % Replace with the correct path to your .tex file
    \end{tabular}
    \caption{The cosine similarity (in degrees), y-axis, between the calculated batch Hessian diagonal and the corresponding optimizer approximations on a \emph{small} batch (124 samples).
    Optimizer updates are denoted on the x-axis.
    Note that these results represent only the Hessian diagonals for the network's \emph{biases}.}
    \label{fig:sappolo-cosine-bias-small-batch}
\end{figure}
\begin{table}[h!]
    \centering
    \caption{Hyperparameter settings for CIFAR-10. Values in parentheses indicate configurations used for individual best-case evaluations. }
    \label{tab:cifar-real-comp}
    \begin{tabular}{lcccccc}  % Adjust the number of 'c's depending on the number of columns
        \toprule
        & \textbf{Learning rate} & \textbf{Weight decay} & \textbf{Beta} & \textbf{Epsilon} & \textbf{Warmup}  \\
        \midrule
        SGD         & $1 \times 10^{-3}$ & $2.5 \times 10 ^{-4} $ & (0.9) & - & 0  \\
        Adam        & $1 \times 10^{-3}$ & $2.5 \times 10 ^{-4} $ & (0.9,0.999) & $1 \times 10^{-08}$ & 0     \\
        AdamW       & $1 \times 10^{-3}$ & $2.5 \times 10 ^{-4} $ \textbf{(0.025)} & (0.9,0.999) & $1 \times 10^{-08}$ & 0 \\
        AdaBelief   & $1 \times 10^{-3}$ & $2.5 \times 10 ^{-4} $ \textbf{(0.025)} & (0.9,0.999) & $1 \times 10^{-08}$ & 0  \\
        RMSProp     & $1 \times 10^{-3}$ & $2.5 \times 10 ^{-4} $ & (0.99) & $1 \times 10^{-08}$ & 0 \\
        Apollo      & $1 \times 10^{-3}$ \textbf{(0.01)} & $2.5 \times 10 ^{-4}$ & (0.9) & $1 \times 10 ^{-4} $ & 0 \textbf{(500)}\\
        ApolloW     & $1 \times 10^{-3}$ \textbf{(0.01)} & $2.5 \times 10 ^{-4} $ \textbf{(0.025)} & (0.9) & $1 \times 10 ^{-4} $ & 0 \textbf{(500)}  \\
        AdaHessian  & $1 \times 10^{-3}$ \textbf{(0.15)} & $2.5 \times 10 ^{-4}$  \textbf{(0.001)} & (0.9,0.999) &$1 \times 10 ^{-4} $& 0 \textbf{(500)} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Hyperparameter settings for TinyImageNet. Values in parentheses indicate configurations used for individual best-case evaluations. }
    \label{tab:tiny-real-comp}
    \begin{tabular}{lcccccc}  % Adjust the number of 'c's depending on the number of columns
        \toprule
        & \textbf{Learning rate} & \textbf{Weight decay} & \textbf{Beta} & \textbf{Epsilon} & \textbf{Warmup}  \\
        \midrule
        SGD         & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} $ & (0.9) & - & 0  \\
        Adam        & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} $ & (0.9,0.999) & $1 \times 10^{-08}$ & 0     \\
        AdamW       & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} \textbf{(0.01)} $ & (0.9,0.999) & $1 \times 10^{-08}$ & 0 \\
        AdaBelief   & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} \textbf{(0.01)}$ & (0.9,0.999) & $1 \times 10^{-08}$ & 0  \\
        RMSProp     & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} $ & (0.99) & $1 \times 10^{-08}$ & 0 \\
        Apollo      & $1 \times 10^{-3}$ \textbf{(0.01)} & $1 \times 10 ^{-4}$ & (0.9) & $1 \times 10 ^{-4} $ & 0 \textbf{(500)}\\
        ApolloW     & $1 \times 10^{-3}$ \textbf{(0.01)} & $1 \times 10 ^{-4} $ \textbf{(0.01)} & (0.9) & $1 \times 10 ^{-4} $ & 0 \textbf{(500)}  \\
        AdaHessian  & $1 \times 10^{-3}$ \textbf{(0.15)} & $1 \times 10 ^{-4}$  \textbf{(0.001)} & (0.9,0.999) &$1 \times 10 ^{-4} $& 0 \textbf{(500)} \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Hyperparameter settings for WMT-14.}
    \label{tab:wmt14-params}
    \begin{tabular}{lcccccc}  % Adjust the number of 'c's depending on the number of columns
        \toprule
        & \textbf{Learning rate} & \textbf{Weight decay} & \textbf{Beta} & \textbf{Epsilon} & \textbf{Warmup}  \\
        \midrule
        SGD         & $1 \times 10^{-3}$ & $1 \times 10 ^{-4} $ & (0.9) & - & 0  \\
        Adam        & $1 \times 10^{-3}$ & 0  & (0.9,0.98) & $1 \times 10^{-09}$ & 4000     \\
        AdamW        & $1 \times 10^{-3}$ & $1 \times 10 ^{-4}$  & (0.9,0.98) & $1 \times 10^{-09}$ & 4000 \\
        AdaBelief   & $1 \times 10^{-3}$ & $1 \times 10 ^{-4}$   & (0.9,0.98) & $1 \times 10^{-09}$ & 4000  \\
        RMSProp     & - & - & - & -& - \\
        Apollo      & 0.04  & 0  & (0.9) & $1 \times 10 ^{-4} $ & 4000  \\
        ApolloW     & 0.04  & $1 \times 10^{-8}$  & (0.9) & $1 \times 10 ^{-4} $ & 4000  \\
        AdaHessian  & 0.1 & 0  & (0.9,0.98) &$1 \times 10 ^{-4} $& 4000 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Hyperparameter settings for curvature approximation quality. Values in parentheses are used in the \emph{SApollo} comparison}
    \label{tab:curve-approx-params}
    \begin{tabular}{lcccccc}  % Adjust the number of 'c's depending on the number of columns
        \toprule
        & \textbf{Learning rate} & \textbf{Weight decay} & \textbf{Beta} & \textbf{Epsilon} \\
        \midrule
        Adam        & $1 \times 10^{-2}$ & 0  & (0.9,0.999) & $1 \times 10^{-08}$   \\
        AdaBelief   & $1 \times 10^{-2}$ & 0   & (0.9,0.999) & $1 \times 10^{-08}$   \\
        Apollo      & $3 \times 10^{-3}$ (0.01)  & 0  & (0.9)  & $1 \times 10 ^{-4} $  \\
        SApollo      & 0.01 & 0  & (0.9)  & $1 \times 10 ^{-4} $  \\

        AdaHessian  & $2 \times 10^{-1}$ & 0  & (0.9,0.999) &   $1 \times 10 ^{-4} $ \\
        \bottomrule
    \end{tabular}
\end{table}
%Nulla fastidii ea ius, exerci suscipit instructior te nam, in ullum
%postulant quo. Congue quaestio philosophia his at, sea odio autem
%vulputate ex. Cu usu mucius iisque voluptua. Sit maiorum propriae at,
%ea cum primis intellegat. Hinc cotidieque reprehendunt eu nec. Autem
%timeam deleniti usu id, in nec nibh altera.



