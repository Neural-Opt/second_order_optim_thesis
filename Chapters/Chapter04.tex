
\chapter{Conclusion and Future Directions}
In this work, we examined \emph{Apollo} and \emph{AdaHessian}, two recent advancements in the field of second-order optimization for neural network training.
We evaluated both optimizers in terms of performance on common datasets and resource consumption,
compared to first-order methods. While \emph{AdaHessian} demonstrated strong performance in approximating the batch Hessian diagonal,
its significant resource requirements and the need for derivative graph creation in PyTorch make it impractical for non-academic use. 
On the other hand, \emph{Apollo}, though able to perform similarly to \emph{AdaHessian} in our experiments,
exhibited a significant limitation in its Hessian approximation due to its clamping operation,
which loses curvature information when values fall below $\sigma$.
Furthermore, we identified several implementation flaws in \emph{Apollo}, which we addressed in our modified version,
\emph{SApollo}.
In addition to fixing these issues, \emph{SApollo} introduces a smoothed version of the Hessian approximation,
$B$, to reduce the influence of stochastic noise. Instead of clamping values below $\sigma$, \emph{SApollo} adds $\sigma$ to approximations
that fall below this threshold, ensuring that small approximation information is retained.
Although this approach led to slightly improved Hessian approximation accuracy and better convergence performance
on the \emph{CIFAR-10} dataset, it remains to be determined whether these performance benefits can be
solely attributed to the correction of implementation errors in \emph{Apollo}, or if assigning similarly small values
for $\sigma$—which is not tunable as a hyperparameter in \emph{Apollo}—could yield the same or even better performance.
Furthermore, we determined that the coupling between the learning rate and $\sigma$ does not hold in practice.
This means that we cannot adjust the influence of $\sigma$ by simply tuning the learning rate in \emph{Apollo},
making the introduction of $\sigma$ as a hyperparameter a necessity.



